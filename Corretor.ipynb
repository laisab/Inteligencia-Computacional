{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lkCo1QTg9jue"
      },
      "outputs": [],
      "source": [
        "# =============================================================================\n",
        "# Lais Aparecida Borges\n",
        "# Projeto: Corretor de Redações para o ENEM com LLM\n",
        "#\n",
        "# Disciplina: CA016IC - Tópicos em Inteligência Computacional\n",
        "#\n",
        "# O ENEM, principal porta de entrada para o ensino superior no Brasil, avalia os\n",
        "# candidatos em diversas competências. A qualidade e a adequação da redação\n",
        "# são fatores determinantes para o sucesso dos estudantes.\n",
        "# No entanto, o acesso a feedback personalizado e detalhado sobre redações pode\n",
        "# ser limitado, especialmente fora de cursos preparatórios.\n",
        "#\n",
        "# Este projeto visa desenvolver um protótipo de corretor de redações para o ENEM\n",
        "# que integra a funcionalidade de Retrieval-Augmented Generation (RAG) com um\n",
        "# LLM finetunado para o contexto da língua portuguesa e o exame. O sistema\n",
        "# receberá um texto de redação e o tema proposto, e utilizará um banco de dados\n",
        "# vetorial contendo comentários de redações anteriores para gerar um feedback\n",
        "# detalhado e contextualizado.\n",
        "#\n",
        "# Escopo e Limitações\n",
        "#\n",
        "# - O corpus de redações utilizado é proveniente do repositório\n",
        "# uol-redacoes-xml.\n",
        "# - A tokenização é realizada pelo tokenizer do modelo Qwen2.5.\n",
        "# - O modelo base é o Qwen2.5-0.5B-PT-BR-Instruct, finetunado com LoRA.\n",
        "# - O RAG é implementado utilizando ChromaDB e Sentence Transformers.\n",
        "# - A avaliação de saída é realizada com métricas ROUGE, BERTScore e BLEU.\n",
        "# - O feedback gerado é um protótipo e pode necessitar de refinamentos para\n",
        "#   cobrir todas as nuances de cada competência do ENEM.\n",
        "#\n",
        "# Repositório de dados de redações: https://github.com/gpassero/uol-redacoes-xml\n",
        "# Modelo pré-treinado: https://huggingface.co/amadeusai/Amadeus-Verbo-FI-Qwen2.5-0.5B-PT-BR-Instruct\n",
        "# Modelo de embeddings: https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
        "# ============================================================================="
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PASSO 1: INSTALAÇÃO DE BIBLIOTECAS E PACOTES\n",
        "# =============================================================================\n",
        "# Instalação do pacote proveniente do repositório que contém um banco de\n",
        "# redações em formato XML, com dados como o tema da redação, o texto produzido,\n",
        "# o texto corrigido, os comentários dos avaliadores, entre outras informações.\n",
        "!pip install git+https://github.com/gpassero/uol-redacoes-xml.git\n",
        "\n",
        "# Instalação das bibliotecas necessárias\n",
        "!pip install -q transformers datasets accelerate peft bitsandbytes trl evaluate chromadb sentence-transformers rouge_score bert_score"
      ],
      "metadata": {
        "id": "PJI6BH1u9tm8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "683596c3-9660-4909-a191-071e5ad22803"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/gpassero/uol-redacoes-xml.git\n",
            "  Cloning https://github.com/gpassero/uol-redacoes-xml.git to /tmp/pip-req-build-fxcomq8u\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/gpassero/uol-redacoes-xml.git /tmp/pip-req-build-fxcomq8u\n",
            "  Resolved https://github.com/gpassero/uol-redacoes-xml.git to commit 94b74fc91c4e7a6b582ebc3708aa0dca2ba12ca6\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: uol_redacoes_xml\n",
            "  Building wheel for uol_redacoes_xml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uol_redacoes_xml: filename=uol_redacoes_xml-0.2-py3-none-any.whl size=2978835 sha256=95372090c5bc8b92b204bda41de3161e6610ba8e7e4a03672d1afce745b43cdf\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8go_bq0j/wheels/c1/e3/ee/70fe667b172b519fa5f401241e6af9b31ab33b05cf715341e5\n",
            "Successfully built uol_redacoes_xml\n",
            "Installing collected packages: uol_redacoes_xml\n",
            "Successfully installed uol_redacoes_xml-0.2\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.9/518.9 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m73.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.0/220.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.7/517.7 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m456.8/456.8 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.21.0 requires opentelemetry-api<=1.37.0,>=1.37.0, but you have opentelemetry-api 1.39.1 which is incompatible.\n",
            "google-adk 1.21.0 requires opentelemetry-sdk<=1.37.0,>=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-exporter-otlp-proto-common==1.37.0, but you have opentelemetry-exporter-otlp-proto-common 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-proto==1.37.0, but you have opentelemetry-proto 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-otlp-proto-http 1.37.0 requires opentelemetry-sdk~=1.37.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\n",
            "opentelemetry-exporter-gcp-logging 1.11.0a0 requires opentelemetry-sdk<1.39.0,>=1.35.0, but you have opentelemetry-sdk 1.39.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PASSO 2: IMPORTAÇÕES NECESSÁRIAS\n",
        "# =============================================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import chromadb\n",
        "import re\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, Trainer, TrainingArguments\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import Dataset\n",
        "from evaluate import load\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.tokenize import word_tokenize\n",
        "from evaluate import load\n",
        "from bert_score import score\n",
        "from chromadb.config import Settings\n",
        "\n",
        "# Bibliotecas necessárias para a base de dados\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "import uol_redacoes_xml"
      ],
      "metadata": {
        "id": "1dgagCNr9yhr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b62d3262-0296-4e21-c686-6d376a422159"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PASSO 3: CRIAÇÃO DO DATAFRAME\n",
        "# =============================================================================\n",
        "# Para evitar erros com o pacote de redações, as informações julgadas\n",
        "# relevantes para o feedback (tema, redação e comentários) são passadas para um\n",
        "# DataFrame, que é salvo localmente.\n",
        "\n",
        "# Carregamento das redações\n",
        "essays = uol_redacoes_xml.load()\n",
        "\n",
        "data = [{\n",
        "    'THEME': essay.prompt.title,\n",
        "    'ESSAY': essay.text,\n",
        "    'COMMENTS': essay.comments\n",
        "} for essay in essays]\n",
        "\n",
        "df_essays = pd.DataFrame(data)\n",
        "df_essays.drop_duplicates(subset='ESSAY', inplace=True)\n",
        "df_essays.dropna(subset=['ESSAY', 'COMMENTS'], inplace=True)\n",
        "\n",
        "# Limpeza de tags HTML/XML nos comentários\n",
        "df_essays['COMMENTS'] = df_essays['COMMENTS'].str.replace(r'</?comments>', '', regex=True)\n",
        "\n",
        "def extract(comment):\n",
        "  match = re.search(r'## Comentário geral\\n(.*?)(?=\\n##|$)', comment, re.DOTALL)\n",
        "  if match:\n",
        "    return match.group(1).strip()\n",
        "  return ''\n",
        "df_essays['COMMENTS'] = df_essays['COMMENTS'].apply(extract)\n",
        "\n",
        "filename = 'Essays.csv'\n",
        "df_essays.to_csv(filename, index=False, encoding='utf-8-sig')"
      ],
      "metadata": {
        "id": "GUj2UY7I90R1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36ca126a-5c9f-44a8-bca1-3f93b71924dd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:uol_redacoes_xml:UOL essays load warnings: \n",
            "WARNING:uol_redacoes_xml:No text  ->  3\n",
            "WARNING:uol_redacoes_xml:Final score != from sum of criteria score  ->  45\n",
            "WARNING:uol_redacoes_xml:Not 5 criteria  ->  7\n",
            "WARNING:uol_redacoes_xml:Total warnings: 55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PASSO 4: CARREGAMENTO DO MODELO PRÉ-TREINADO\n",
        "# =============================================================================\n",
        "model_id = \"amadeusai/AV-FI-Qwen2.5-0.5B-PT-BR-Instruct\" # Especializado na língua portuguesa\n",
        "\n",
        "# Tokenização do modelo\n",
        "tok = AutoTokenizer.from_pretrained(model_id)\n",
        "if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    dtype=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "model"
      ],
      "metadata": {
        "id": "RFqHRboJ92Qv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880,
          "referenced_widgets": [
            "a59b9195ce59418ea9cc3435d72174b0",
            "253fa9b0ef074a0fa0107501516dcbf0",
            "a2ea2aee2fff40d7bd12d1f02c2f8e23",
            "a90049ef4dd6463fb69d6aaa18d5de83",
            "ce2d8bd9171140a28b07b5b6e661d846",
            "ddc73e9376314fb9ae691b19717f494b",
            "7236509818784a3aa065847dcf65a9b5",
            "98f7a1fa19ab48e99c7d11221bb114da",
            "544e83782f5d46c1b9c3673eb03f4cbb",
            "33298fc33fbc4487a4ebf4ae9957808a",
            "b2c268705080420590769a99fb36acc9",
            "af12627cdfa14b40a8afe2ef3ef8089a",
            "91ddf705034e457e8bedc5b03a1d74e4",
            "29d9c913302d4654bc8910bd8f6de9ba",
            "5ebac3c61a884fec8b3adf2a2a943eb8",
            "6f3270cdaf2d4882a2e47e6f86f7e223",
            "412b2bccf2f540d090148c6cf2aefcf5",
            "aefdda878b4c4244a87a353a1c78304d",
            "4c7f878d30154486bda115b683f16fbb",
            "39527500c9a94c5090b4bdc167af9d84",
            "2a249716bde24fa7a7d18a7d5772db6c",
            "a2cffb938a684daba339b6344500ccad",
            "661efcfd642a41f697977e5c3e90a105",
            "2042ab1a94574c779016f3733f28820c",
            "f0ee3695c5b247db9cb040d891507ad2",
            "f079a9334404488393827fe2bbd3bb30",
            "28f18bafbfc046948b19e255b7eca80d",
            "05f1b7d80a894af3a14b71816001e6ac",
            "4cc254361e624eaf8dd990a934f92cb4",
            "445f50791aa149a38a8cd08cf0264238",
            "fcebe0f96e38432ba5af66f1e0886836",
            "5a92fbbca82e4ceeaf66e398865870dc",
            "9f4209560a2940ef9343d8f4254215a7",
            "34d4a0fef1c548f78be151f8623321eb",
            "d1a869795ff24dc2bac746d9f592f591",
            "7327c2896f434e0b9f4df3edc5ce0f93",
            "17318e2e47d04322a3d1641a924cdb5d",
            "cbf229237e16413f8586821b3a3ea0ce",
            "2030f2899c51498a9fae71b26ea6eb27",
            "b28247d47e604227b9fe75746e697f35",
            "dad07619ea2c4ac8a61f8509331579a5",
            "96d4d8d733ac420cbbc1e62e9b8dec52",
            "21a81cee7b1f414daf8fcd66f46f8f36",
            "03a64c9d43b14ce6bf51f55f24aa88c4",
            "a919a0c22ac94288850a81b7f005f8ab",
            "a30899d4beb8456f841d4c91fcf4f2e4",
            "2c01b8b41bfa40709b4b8984fbdc528a",
            "ea0347f2e26f415d90b9ecd145f9901c",
            "eb2f0b25d6a54644a7fab6cde4490560",
            "87dae14385a94f4db68401bc64e950a3",
            "56477826fb2c4afba7034fd44d76fefc",
            "2d116727da794b66877eb7d677e95ecc",
            "176edbe2d361445f96dfce1d9045b238",
            "e6fcd765ffdf4ce78676c03700904033",
            "ab5028d34e2a4b5d84c057fc357a14a4",
            "35df725956644cc08437ded509fc5b3f",
            "e8c8a90ccaaa435e94ee50e3569bb9c8",
            "678f5cf4a8ec4a509d7dc461e9db93e8",
            "33fd9f7eebeb4dfdae3fb453db2c3b24",
            "ec65fbee6ef7438d9d902f5d59191990",
            "6e2136d8330944d5b6bf30b6201cbb01",
            "210fdfbaf40e4724baab36c2bcc66aad",
            "e20a524037754b28847c2f8dcdc4a4be",
            "22bac428957c444fb7ab5e4ccb1c9a92",
            "ee4c7d97fea64fc2bdd36b3cb0374a68",
            "77d1d031cf8142a48565a66b660e81b6",
            "792e97bf974a401db99a6482419bc3a4",
            "dfe0d8dcb4044d5d86431fbd8e752446",
            "c5545b5ab93b49729de9f34ee3f2ba47",
            "4643a4874e5747ccb154cb5ce34088e5",
            "9556bc6df7e947928946fa6b9a4c5a7d",
            "9ad284c1087044bd9ec794ec28b6e941",
            "8186934e75b943faa1315b174bb1c432",
            "18671ce9475144008d75a274675d2dc0",
            "45089e70e28c4cad89996bb4419ead81",
            "9f6211a4ac474302aa1340cbd2758eba",
            "4d31b685a39d4a188e183db9a1aee8aa",
            "7755b93a95774427b5c8e2fbe3d2b1e9",
            "067271c282c947888dda4051f8a2d1aa",
            "e26c88122862474087841c94ebbba621",
            "07734b18f1624510831d64dc596187fb",
            "0e674c467afd41bb859fd64974d4f107",
            "c67c39ff81f14b90995501408eaa968d",
            "73df1a33c0274d7aa65655923410522d",
            "94fd010aabd34af8a4380ccb0693d001",
            "379a0796e3a6419f91a8a791ff24fb01",
            "21f8844a812f4193b50d4687786af594",
            "90a758e73c4e4022a5484e3de4f95f59",
            "ca9bca6950444ac2b9ba1ab1bbdba1b0",
            "ff1987d3bf664d47b06de775c84746b7",
            "1ca37b8c34d1400fb22a1483266b7519",
            "4f7bfea7dc9d4c9ea523a62e3866105a",
            "e21a47919c4f45219b3e22697394c584",
            "7ef7cb1096764da299cd596f0239fce4",
            "1f89b96c57d146e9a9ffd534c6ac9cc0",
            "dc6851bb946743578ed39e6e181b7c39",
            "ee8f63ed1a154fb8b8e5e10f2cc0bc9e",
            "7d9fa7d2203e459985ae62e957e05118",
            "a3eaffdb8f6a4032bb397d27ec1adc11"
          ]
        },
        "outputId": "2ae4b70d-d475-45da-e5fc-d431a3747a4e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a59b9195ce59418ea9cc3435d72174b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af12627cdfa14b40a8afe2ef3ef8089a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "661efcfd642a41f697977e5c3e90a105"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34d4a0fef1c548f78be151f8623321eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/605 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a919a0c22ac94288850a81b7f005f8ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/499 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35df725956644cc08437ded509fc5b3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/778 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "792e97bf974a401db99a6482419bc3a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7755b93a95774427b5c8e2fbe3d2b1e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/246 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca9bca6950444ac2b9ba1ab1bbdba1b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 896)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
              "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
              "          (act_fn): SiLUActivation()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PASSO 5: CONFIGURAÇÃO DO LORA\n",
        "# =============================================================================\n",
        "lora_cfg = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_cfg)\n",
        "model"
      ],
      "metadata": {
        "id": "3sBsTjQ797R0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "939537d4-18eb-4130-8c78-f40691faf17c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Qwen2ForCausalLM(\n",
              "      (model): Qwen2Model(\n",
              "        (embed_tokens): Embedding(151936, 896)\n",
              "        (layers): ModuleList(\n",
              "          (0-23): 24 x Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=896, out_features=896, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=896, out_features=128, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=896, out_features=128, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=128, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=896, out_features=896, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=896, out_features=4864, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4864, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=896, out_features=4864, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=896, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4864, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4864, out_features=896, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4864, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=896, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        (rotary_emb): Qwen2RotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PASSO 6: CARREGAMENTO DO MODELO DE EMBEDDINGS\n",
        "# =============================================================================\n",
        "# Carregamento do modelo de embedding capaz de compreender\n",
        "# a intenção e o contexto em português.\n",
        "embedding_model_id = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
        "embedding_model = SentenceTransformer(embedding_model_id)\n",
        "embedding_model"
      ],
      "metadata": {
        "id": "4xVJ0pAy9_XT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424,
          "referenced_widgets": [
            "e60f1f3cffe9410e8ca3408ff53750a4",
            "61e417aa1d7b4a52bd33c9609485b254",
            "d4f75bff8cd24e7a921a198b866455ed",
            "682ca41bf2474433a18b480be14f9fa7",
            "5e093f7c9cf849da8d597111397b1512",
            "332181db432e46fc8789cde89b18d842",
            "9ca813cf0fa7470099471d7d3f5c790a",
            "623c8a37ce8b42519c51374d7da50dda",
            "a322b689ef774ef4aca34f9ce9231307",
            "78e50c34d9ce4ceb9bd0455e824397b7",
            "bc352e65f0464e099e8fe5424081b14c",
            "b47ea6c0b9c34a49970d1a551a7fbec3",
            "8b706b5e3dc8486b8b370ed4dc276892",
            "5129e8ead69244ca82f84aabdbb538d3",
            "a818f3b6f97d494880d505871eabfedf",
            "26e420633cc24a92b3d4e1223e8a52eb",
            "f7fd7b043891409cb82e8af7401d5c55",
            "5a40b2bcee254fa598501132d3f7cd3c",
            "238a4f94b74448d296312341ae02e566",
            "86f6c6360a9e4c52b96d14b7499ee8ed",
            "ce34ad3bc7994fd4a553d2bd43abbbf4",
            "6b49931475fd4aa29fc340587aa891bf",
            "e6d7cf1149424e4fa7e4c717d345facd",
            "3f4182f4906e4038a47357bb70043cb9",
            "974dc273e0b64783aa7a5b4f4ef475b0",
            "28ee90a638d44fa694b49d907da812c0",
            "29ac8e7cd84545138a321943fb27f510",
            "abe8dd6f6a1149dea16cd7c64a3f790f",
            "00c1ee857ef848e58f75df4995d84cce",
            "da4636d5e88c42cf9084d2aa2e5088d3",
            "f9b9d1d8f7a94e489cc69b3dcfad3705",
            "500551d985b34b2fbbb9612969c4572d",
            "80bb698befcc456fabcfcc754f6130cf",
            "83db9c0adabf4c50b38d8a8ea35922a7",
            "a935cda5257e420f81e640b513e6a099",
            "ab854d98d3df495b8ffc8fb2d397fc9d",
            "90dd66a4673447e38a6797f9c98617b8",
            "0b0c3507345e45698f2d5d97acf5b009",
            "c21c3d80633c484e82a1d6f7ddbe60bc",
            "1a32643926e941d68256ea137faf8869",
            "1de09f936cff45e0858d7a3e83a97482",
            "f367f17b9bfb457ca9a453bcc3d8c45f",
            "16e7252e8c4c4adf86f6606b42d0b80f",
            "ae5ebbb88066480d9239d566df8212b7",
            "f28d7bf1708c4faeb12a3e09b8b56186",
            "5fc39d4e51f5437e8899224da832f8cf",
            "5e49a754938e4b1eb7d1b24d8815d6ad",
            "fda55762def04576ba1c665140e0a1d4",
            "35cb415f84a64799a3cbede6bf0f7f83",
            "65f83697e733454e8c49f04a21e247df",
            "a4ef12b50aca4b3b9572ec5761d9372d",
            "e3ec187436de44afbb838200014e22d7",
            "f699e770de5d4d9ebeed53f62a43a34d",
            "8815942bc63247ae8480b2df48bc6623",
            "e653bb3966974027a76446bb6ca1289c",
            "b47aa0a69e10476882355b35b9a96a2e",
            "3697a70c9ab545c5bdb916aaf334acfb",
            "886e536f2a4a4971906d08f72b90ea4f",
            "1e998bc1a7334c629fe1e9eff1cadcd6",
            "0cdf06c1a0204ad8b8cda11edaa36373",
            "f20826e13637452cb2f9e4ffcd849803",
            "e9397ebf30b949d39acfd2c5dae2d9b8",
            "393bf146fb1949e98ee5e2bb9f9d4be6",
            "64401d7c8bad45299d16a3932f4c9873",
            "27a6422ff3794aed8f561f4df9828287",
            "86ef504a4b3843eaace48c936fcabb65",
            "cdf21e06e13447699175ce36267607f5",
            "c5b5f7390bc74e78bf02c5fbdc90463c",
            "284773c84adf4de8bd63c9ebe938a39c",
            "261ff56e3c0847debb2034b0c06e045a",
            "5ce752d8327447bd8b9bf3568335721b",
            "3cba25b0dda84ce48bde3a19dacc7bbc",
            "a350eabae084423eaf7c7b52ef2a31b7",
            "7eb52071c9ce46ca889631d62b6b1d69",
            "ddbde3680e8f40e1a07adde4d8fd13a9",
            "552743811ec64738b37431f657a5039c",
            "c5b5990500654042b1c51f9618cde966",
            "3b824c7ce13d466db55b849180ffb868",
            "eddbabd9f9414f8db0837edf5bd2ff7f",
            "c20c8d87539d408296fc481b24b1924e",
            "df094c49563e4554ba81bd532b1a92cb",
            "c3b5bca7e1064949b40e8f51efd089d0",
            "1b7d3d18253b42198591fc7090c9eea3",
            "75c3776b21d6400b987f14c6cc5bdd2d",
            "3e52751d54ad41c598ff3b52be194951",
            "8cf8ef804a9f4c3084a5f3ec40f59697",
            "32761e72f59349f9a7257f3cf53d6944",
            "30a2b3eb574b4570a411052b2b39f356",
            "5135d91c3ea04d1b9fb1a5ea03546bc1",
            "ec11d264fae24bf286fed4891f81e93b",
            "14860d7dae314ce4b9ba83b9d6da1487",
            "aedb938fea8743b98da80af66ee30e56",
            "88b93deeca3f4b28a59450f11f193ca4",
            "3742af1896ed4a2a89bd5ede544d0766",
            "13fc94dd208a4ed1b8cb9fa83c759a18",
            "e3972acd08384b569ca78f128ccd8dfc",
            "ab4341af827b4cebaf623d6229573f76",
            "88617c09aa304a48a056108f13a10a14",
            "45dacc759aba4598b3d1e909f7280ff2",
            "bd6b9d899f0f49aeab4a8924534c9726",
            "91feacd4a3d2457ba04e7ccf401348dd",
            "7cf33751fe77454d91861c7d0d20be09",
            "327ded9737ab434281a9783454f12b4b",
            "3bc30c3c7a154c679c7ec434afbcc906",
            "4f98c4e10fd1428ab5dfb92ad3a496bd",
            "c9bcf57f07254903822ba5d21ae957fe",
            "118ca96e93ba4d93afb1d8c7c7c1a497",
            "b173d0763708499d87a488f601e3c4a9",
            "c80ab3f943c846f88d56c35a1c4e75de",
            "47c114e4e79e4309814938577795e6e7"
          ]
        },
        "outputId": "6639bf96-9f77-4298-9dc5-0bd8eb89866d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e60f1f3cffe9410e8ca3408ff53750a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b47ea6c0b9c34a49970d1a551a7fbec3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6d7cf1149424e4fa7e4c717d345facd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83db9c0adabf4c50b38d8a8ea35922a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/645 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f28d7bf1708c4faeb12a3e09b8b56186"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/471M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b47aa0a69e10476882355b35b9a96a2e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdf21e06e13447699175ce36267607f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b824c7ce13d466db55b849180ffb868"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5135d91c3ea04d1b9fb1a5ea03546bc1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bd6b9d899f0f49aeab4a8924534c9726"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SentenceTransformer(\n",
              "  (0): Transformer({'max_seq_length': 128, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
              "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PASSO 7: PREPARAÇÃO DO BANCO DE DADOS VETORIAL\n",
        "# =============================================================================\n",
        "# Configuração do ChromaDB\n",
        "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
        "\n",
        "# Criação da coleção\n",
        "collection_name = \"essays_feedback\"\n",
        "collection = chroma_client.get_or_create_collection(name=collection_name)"
      ],
      "metadata": {
        "id": "hYVfs6-V-Bbr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para gerar embeddings e adicionar ao ChromaDB\n",
        "def add_to_chroma(documents: list[str], metadatas: list[dict], ids: list[str]):\n",
        "  \"\"\"\n",
        "  Adiciona documentos ao ChromaDB.\n",
        "\n",
        "  Args:\n",
        "    documents (list[str]): Lista dos documentos para adicionar.\n",
        "    metadatas (list[dict]): Lista dos metadados associados aos documentos.\n",
        "    ids (list[str]): Lista dos IDs associados aos documentos.\n",
        "\n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "  # Geração dos embeddings\n",
        "  embeddings = embedding_model.encode(documents, batch_size=32, show_progress_bar=True).tolist()\n",
        "\n",
        "  # Adição dos embeddings ao ChromaDB\n",
        "  print(f\"\\nAdding {len(documents)} documents to ChromaDB...\")\n",
        "  collection.add(\n",
        "      embeddings=embeddings,\n",
        "      documents=documents,\n",
        "      metadatas=metadatas,\n",
        "      ids=ids\n",
        "  )\n",
        "  print(f\"{len(documents)} documents added to collection '{collection_name}'\")\n",
        "\n",
        "# Preparação dos dados\n",
        "docs_to_index = df_essays['ESSAY'].tolist()\n",
        "metadatas_to_index = df_essays.apply(lambda row: {\n",
        "    \"theme\": row['THEME'],\n",
        "    \"comments\": row['COMMENTS']\n",
        "}, axis=1).tolist()\n",
        "ids_to_index = [f\"essay_comment_{i}\" for i in range(len(df_essays))]\n",
        "\n",
        "add_to_chroma(docs_to_index, metadatas_to_index, ids_to_index)"
      ],
      "metadata": {
        "id": "WHst_Kta-C3r",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "acc00896d8114356aac965f0bcece036",
            "c6ead47009d54259a328b18b97f6f61e",
            "afc31df4ae8e4a4b90ebb4c8ce24368a",
            "3ea5853d873a4056b0a1559045b53f1c",
            "b6e89b47d293476a9239841a73c1ed16",
            "0463a3d167ba4d8ca4e4be49a3c49afa",
            "a3c040f13385477f9a9528901bd1792c",
            "a2232e4bb4a5435ab68ca8246de9a422",
            "a7d82904216740629f75ff8da5780d51",
            "43ba7a45f5104902aa58d6735f61aa04",
            "2520b11a39154893bd368ccd48b08be8"
          ]
        },
        "outputId": "25c373ed-80d4-4ee4-fbcd-49339ab321be"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/68 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acc00896d8114356aac965f0bcece036"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Adding 2162 documents to ChromaDB...\n",
            "2162 documents added to collection 'essays_feedback'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Função para buscar informações relevantes\n",
        "def search_chroma(query_text: str, n_results: int) -> list[str]:\n",
        "  \"\"\"\n",
        "  Busca os documentos mais relevantes.\n",
        "\n",
        "  Args:\n",
        "    query_text (str): Texto de consulta.\n",
        "    n_results (int): Número de resultados a serem retornados.\n",
        "\n",
        "  Returns:\n",
        "    list[str]: Lista dos documentos mais relevantes.\n",
        "  \"\"\"\n",
        "  # Geração do embedding da consulta\n",
        "  query_embedding = embedding_model.encode(query_text).tolist()\n",
        "\n",
        "  # Consulta ao ChromaDB para encontrar documentos mais relevantes com base na similaridade dos embeddings\n",
        "  results = collection.query(\n",
        "      query_embeddings=[query_embedding],\n",
        "      n_results=n_results,\n",
        "      include=['documents', 'metadatas']\n",
        "  )\n",
        "\n",
        "  relevant_docs = []\n",
        "  if results and results.get('documents') and results['documents'][0]:\n",
        "    for doc, meta in zip(results['documents'][0], results.get('metadatas', [{}])[0]):\n",
        "      relevant_docs.append({\"text\": doc, \"metadata\": meta})\n",
        "\n",
        "  return relevant_docs"
      ],
      "metadata": {
        "id": "BaDrbZ3F-Eh7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PASSO 8: PREPARAÇÃO DOS DADOS\n",
        "# =============================================================================\n",
        "PROMPT = \"\"\"Você é um assistente especializado em dar feedback para redações do ENEM. Utilize os exemplos de feedbacks reais abaixo para guiar seu estilo e critérios de avaliação.\n",
        "\n",
        "Exemplos: {contexto}\n",
        "\n",
        "Redação para Avaliação:\n",
        "Tema: {tema}\n",
        "Texto: {redacao}\n",
        "\n",
        "Feedback:\n",
        "\"\"\"\n",
        "\n",
        "# Conversão do DataFrame para um Hugging Face Dataset\n",
        "ds = Dataset.from_pandas(df_essays)\n",
        "ds"
      ],
      "metadata": {
        "id": "nZH89TW3-GXO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0809625-2cf8-4d67-f90f-e1b06d7cf9af"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['THEME', 'ESSAY', 'COMMENTS', '__index_level_0__'],\n",
              "    num_rows: 2162\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PASSO 9: TOKENIZAÇÃO\n",
        "# =============================================================================\n",
        "# Tamanho máximo da sequência\n",
        "MAX_LEN = 800\n",
        "\n",
        "# Função de tokenização que processa cada exemplo\n",
        "def tok_example(example):\n",
        "  \"\"\"\n",
        "  Tokeniza um exemplo de dados.\n",
        "\n",
        "  Args:\n",
        "    example (dict): Exemplo de dados.\n",
        "\n",
        "  Returns:\n",
        "    dict: Exemplo de dados tokenizados.\n",
        "  \"\"\"\n",
        "  prompt = PROMPT.format(\n",
        "      contexto=\"Nenhum exemplo\",\n",
        "      tema=example['THEME'],\n",
        "      redacao=example['ESSAY']\n",
        "  )\n",
        "\n",
        "  tp = tok(prompt, add_special_tokens=False)\n",
        "  tr = tok(example['COMMENTS'] + tok.eos_token, add_special_tokens=False)\n",
        "\n",
        "  input_ids = (tp['input_ids'] + tr['input_ids'])[:MAX_LEN]\n",
        "  attention_mask = [1] * len(input_ids)\n",
        "  labels = ([-100]*len(tp[\"input_ids\"]) + tr[\"input_ids\"])[:MAX_LEN]\n",
        "\n",
        "  return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "train_ds = ds.map(tok_example, remove_columns=ds.column_names)\n",
        "train_ds"
      ],
      "metadata": {
        "id": "ZQsCyOeR-Hpi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "a3a97ec7a6044bc5ad6d34ecb83a1bb8",
            "508c8a82fdef424baf0dc417cede412b",
            "93c662459d534519bde53707b73000e3",
            "4d4867cb2523487787c9ef522e46c484",
            "fa15ea118be249c7a04dd0296caa6b97",
            "c54390d181a5411199b814b0f035ea9b",
            "30c971a02d96487cb30f9937679d981e",
            "5fb099cd5e5840fab0a5be5600e9acae",
            "b0cfe64768d94bb59d13123d0dfc2bde",
            "65864db4392a44828aa0ee34ad47aaca",
            "60f95e5cf27c4852ae76fcc9f3160e44"
          ]
        },
        "outputId": "030587ed-36ef-4889-da42-c4c848bec0ea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/2162 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a3a97ec7a6044bc5ad6d34ecb83a1bb8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 2162\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PASSO 10: BATCH\n",
        "# =============================================================================\n",
        "def collate(batch):\n",
        "  \"\"\"\n",
        "  Função de processamento de lote.\n",
        "\n",
        "  Args:\n",
        "    batch (list): Lista de exemplos de dados.\n",
        "\n",
        "  Returns:\n",
        "    dict: Exemplo de dados processados.\n",
        "  \"\"\"\n",
        "\n",
        "  # Função auxiliar para a organização das redações\n",
        "  def pad_list(lst, pad_value):\n",
        "    maxlen = max(len(x) for x in lst)\n",
        "\n",
        "    out = []\n",
        "    for x in lst:\n",
        "      t = torch.tensor(x, dtype=torch.long)\n",
        "\n",
        "      if t.size(0) < maxlen:\n",
        "        t = torch.nn.functional.pad(t, (0, maxlen - t.size(0)), value=pad_value)\n",
        "      out.append(t)\n",
        "\n",
        "    return torch.stack(out)\n",
        "\n",
        "  ids = [b[\"input_ids\"] for b in batch]\n",
        "  att = [b[\"attention_mask\"] for b in batch]\n",
        "  lab = [b[\"labels\"] for b in batch]\n",
        "\n",
        "  return{\n",
        "    \"input_ids\": pad_list(ids, tok.pad_token_id),\n",
        "    \"attention_mask\": pad_list(att, 0),\n",
        "    \"labels\": pad_list(lab, -100)\n",
        "  }"
      ],
      "metadata": {
        "id": "T0dhYoxV-JQa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PASSO 11: CONFIGURAÇÃO DO TREINAMENTO\n",
        "# =============================================================================\n",
        "# Criação de um batch com 12 exemplos para ajudar no aprendizado do modelo\n",
        "# de uma maneira mais estável com per_device_train_batch_size=3 e\n",
        "# gradient_accumulation_steps=4.\n",
        "\n",
        "# Aumento gradual da velocidade de aprendizado no início e, depois, diminuição\n",
        "# suave com lr_scheduler_type=\"cosine\" e warmup_steps=50.\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"./essay\",\n",
        "    max_steps=350, # Controle da duração do treino\n",
        "    per_device_train_batch_size=3,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=2e-5, # Velocidade de aprendizado mais segura\n",
        "    fp16=True,\n",
        "    group_by_length=True, # Organização dos exemplos de redações por tamanho\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_steps=50,\n",
        "    logging_steps=50,\n",
        "    save_steps=100,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=model, args=args, train_dataset=train_ds, data_collator=collate)"
      ],
      "metadata": {
        "id": "oPV1GscU-K1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8cda2be-3634-4265-aed7-b5f18d961689"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PASSO 12: TREINAMENTO\n",
        "# =============================================================================\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "O2pKJtmV-Pfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "outputId": "ef08265c-1d71-4488-f2cf-8f71c249f1f8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='350' max='350' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [350/350 14:02, Epoch 1/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.780400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.381400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.323100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.265500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.273600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.251200</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=350, training_loss=2.395735015869141, metrics={'train_runtime': 847.6814, 'train_samples_per_second': 4.955, 'train_steps_per_second': 0.413, 'total_flos': 5553220467767040.0, 'train_loss': 2.395735015869141, 'epoch': 1.9375866851595007})"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PASSO 13: GERAÇÃO\n",
        "# =============================================================================\n",
        "# Antes da busca, a redação é truncada para prevenção de erros de dimensão no\n",
        "# modelo de embedding, focando no início/corpo do texto, o que garante que o\n",
        "# contexto recuperado seja semanticamente relevante.\n",
        "def generate(instr):\n",
        "  \"\"\"\n",
        "  Função para gerar feedback a partir de uma redação.\n",
        "\n",
        "  Args:\n",
        "    instr (dict): Dicionário contendo o tema e a redação.\n",
        "\n",
        "  Returns:\n",
        "    generated_text (str): Feedback gerado.\n",
        "  \"\"\"\n",
        "  # Busca pelos trechos de texto mais relevantes relacionados à redação em questão\n",
        "  max_query_tokens = 256 # Limite para a consulta de busca\n",
        "  tokens_obj = tok(instr['ESSAY'], max_length=max_query_tokens, truncation=True, return_tensors=\"pt\", add_special_tokens=False)\n",
        "  search_query = tok.decode(tokens_obj['input_ids'][0], skip_special_tokens=True)\n",
        "  relevant_contexts = search_chroma(search_query, 3) # 3 trechos mais relevantes\n",
        "\n",
        "  context_text = \"\"\n",
        "  for i, doc in enumerate(relevant_contexts):\n",
        "    context_text += f\"\\n[Exemplo {i+1}]\\nFeedback Real: {doc['metadata']['comments']}\\n\"\n",
        "\n",
        "  prompt = PROMPT.format(\n",
        "      contexto=context_text,\n",
        "      tema=instr['THEME'],\n",
        "      redacao=instr['ESSAY']\n",
        "  )\n",
        "\n",
        "  x = tok(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(model.device)\n",
        "\n",
        "  # Geração da resposta\n",
        "  with torch.no_grad():\n",
        "    y = model.generate(\n",
        "        **x,\n",
        "        max_new_tokens=1024,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "        temperature=0.4,\n",
        "        eos_token_id=tok.eos_token_id,\n",
        "        pad_token_id=tok.pad_token_id,\n",
        "        no_repeat_ngram_size=4, # Força a diversidade vocabular, impedindo loops de frase comuns\n",
        "        repetition_penalty=1.2 # Força a diversidade semântica\n",
        "    )\n",
        "\n",
        "    # Processamento da saída\n",
        "    input_len = x[\"input_ids\"].shape[1]\n",
        "    generated_tokens = y[0][input_len:]\n",
        "    generated_text = tok.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "    print(\"=\"*40)\n",
        "    print(f\"TEMA: {instr['THEME']}\")\n",
        "    print(\"-\" * 40)\n",
        "    print(\"FEEDBACK:\\n\")\n",
        "    print(generated_text)\n",
        "    print(\"=\"*40)\n",
        "\n",
        "  return generated_text"
      ],
      "metadata": {
        "id": "anrEaFy9-Qvb"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Teste\n",
        "example = df_essays.iloc[25]\n",
        "generated_text = generate(example)"
      ],
      "metadata": {
        "id": "bMQrB9Dy-SJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae8311ee-b56f-4e79-bb11-6d795565eb96"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "TEMA: É preferível praticar ou sofrer uma injustiça?\n",
            "----------------------------------------\n",
            "FEEDBACK:\n",
            "\n",
            "Apesar de ter algumas pequenas dificuldades, o texto demonstrou interesse pela tese, mesmo quando falava sobre injustiça e injustiça; também houve algum desenvolvimento nos aspectos teóricos, principalmente na análise de alguns casos específicos. Mas a partir dessa situação inicial, o aluno ficaria perdido na dissertação, porque ele nunca conseguiu definir o problema, construir uma defesa e mostrar como o comportamento humano afeta a distribuição da justiça.\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Texto original\n",
        "print(\"=\"*40)\n",
        "print(f\"TEMA: {example['THEME']}\")\n",
        "print(\"-\" * 40)\n",
        "print(\"FEEDBACK ORIGINAL:\\n\")\n",
        "print(example['COMMENTS'])\n",
        "print(\"=\"*40)"
      ],
      "metadata": {
        "id": "vWMLUOFB-aqL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10064011-3c26-4cce-efb4-2f32e8b39806"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "TEMA: É preferível praticar ou sofrer uma injustiça?\n",
            "----------------------------------------\n",
            "FEEDBACK ORIGINAL:\n",
            "\n",
            "Comentário geral\n",
            "O texto começa mal, mas melhora no segundo parágrafo. No entanto, é basicamente expositivo, sem apresentar claramente sua resposta à pergunta do tema, nem defendê-la com argumentos. Basicamente, o autor tenta explicar o que é justiça/injustiça, dando exemplos variados de injustiça. Apenas no último parágrafo, reflete que não está em poder de ninguém evitar ser vítima de injustiças, no entanto, qualquer pessoa com consciência pode evitar praticá-las. Infelizmente, isso não chega a cumprir os requisitos da proposta de redação.\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================================\n",
        "# PASSO 14: AVALIAÇÃO\n",
        "# =============================================================================\n",
        "rouge = load(\"rouge\")\n",
        "\n",
        "def evaluate(reference_text, generated_text, lang=\"pt\"):\n",
        "  \"\"\"\n",
        "  Função para avaliar a saída do modelo.\n",
        "\n",
        "  Args:\n",
        "    reference_text (str): Texto de referência.\n",
        "    generated_text (str): Texto gerado.\n",
        "    lang (str): Idioma dos textos.\n",
        "\n",
        "  Returns:\n",
        "    rouge_results (dict): Resultados do ROUGE.\n",
        "    bert_precision (torch.Tensor): Precisão do BERT.\n",
        "    bert_recall (torch.Tensor): Revocação do BERT.\n",
        "    bert_f1 (torch.Tensor): F1\n",
        "    bleu_score (float): BLEU.\n",
        "  \"\"\"\n",
        "  references = [reference_text]\n",
        "  predictions = [generated_text]\n",
        "\n",
        "  # ROUGE\n",
        "  rouge_results = rouge.compute(predictions=predictions, references=references)\n",
        "  # BERTScore\n",
        "  bert_precision, bert_recall, bert_f1 = score(references, predictions, lang=lang, verbose=False)\n",
        "  # BLEU\n",
        "  bleu_score = sentence_bleu(references, generated_text)\n",
        "\n",
        "  return rouge_results, bert_precision, bert_recall, bert_f1, bleu_score"
      ],
      "metadata": {
        "id": "clPfxObU-dhk"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_results, bert_precision, bert_recall, bert_f1, bleu_score = evaluate(example['COMMENTS'], generated_text)\n",
        "\n",
        "print(\"=\"*40)\n",
        "print(\"   AVALIAÇÃO\")\n",
        "print(\"ROUGE Scores:\")\n",
        "print(f\" ROUGE-1: {rouge_results['rouge1']:.3f}\")\n",
        "print(f\" ROUGE-2: {rouge_results['rouge2']:.3f}\")\n",
        "print(f\" ROUGE-L: {rouge_results['rougeL']:.3f}\")\n",
        "print(\"-\"*40)\n",
        "print(\"BERT Scores:\")\n",
        "print(f\" Precision: {bert_precision.mean().item():.3f}\")\n",
        "print(f\" Recall:    {bert_recall.mean().item():.3f}\")\n",
        "print(f\" F1:        {bert_f1.mean().item():.3f}\")\n",
        "print(\"-\"*40)\n",
        "print(f\"BLEU Score: {bleu_score:.3f}\")\n",
        "print(\"=\"*40)"
      ],
      "metadata": {
        "id": "zX5MV8G4-fBK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ea2d41-b8e3-475e-adfb-22223a057245"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "   AVALIAÇÃO\n",
            "ROUGE Scores:\n",
            " ROUGE-1: 0.234\n",
            " ROUGE-2: 0.047\n",
            " ROUGE-L: 0.129\n",
            "----------------------------------------\n",
            "BERT Scores:\n",
            " Precision: 0.685\n",
            " Recall:    0.713\n",
            " F1:        0.699\n",
            "----------------------------------------\n",
            "BLEU Score: 0.376\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As métricas indicam que o modelo captou a semântica, mas utilizou palavras e estruturas bem diferentes.\n",
        "\n",
        "* **ROUGE**: o valor obtido desta métrica demonstra que o modelo reescreveu a crítica usando um vocabulário próprio.\n",
        "* **BERT**: o valor de F1 indica que o modelo entendeu a essência da crítica original.\n",
        "* **BLEU**: indica que existe correspondência de termos-chave, mas não de frases inteiras.\n",
        "\n",
        "O texto gerado foi um pouco mais \"gentil\" que o original, o que explica os valores de precisão e recall, demonstrando que ele tende a ser mais construtivo e/ou diplomático na linguagem."
      ],
      "metadata": {
        "id": "wH-nfZnf3T-w"
      }
    }
  ]
}