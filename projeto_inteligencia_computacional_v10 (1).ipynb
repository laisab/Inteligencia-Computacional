{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNlXSXiE2Pd8KYCArlnfinc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"UEzCkEHmcQF2"},"outputs":[],"source":["!pip install -q transformers datasets accelerate peft bitsandbytes trl evaluate\n","!pip install git+https://github.com/gpassero/uol-redacoes-xml.git"]},{"cell_type":"code","source":["import torch\n","import nltk\n","nltk.download('punkt_tab')\n","import uol_redacoes_xml\n","import pandas as pd\n","from transformers import AutoTokenizer, Trainer, TrainingArguments, AutoModelForCausalLM\n","from peft import LoraConfig, get_peft_model\n","from datasets import Dataset\n","from nltk.translate.bleu_score import sentence_bleu\n","from nltk.tokenize import sent_tokenize, word_tokenize"],"metadata":{"id":"-vQp4jufcXm9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["essays = uol_redacoes_xml.load()\n","data = []\n","\n","for essay in essays:\n","  if essay.comments.find('<comments>') or essay.comments.find('</comments>'):\n","    essay.comments = essay.comments.replace('<comments>', '')\n","    essay.comments = essay.comments.replace('</comments>', '')\n","\n","  data.append({\n","      'THEME': essay.prompt.title,\n","      'ESSAY': essay.text,\n","      'COMMENTS': essay.comments\n","  })\n","\n","filename = 'Essays.csv'\n","df_essays = pd.DataFrame(data)\n","df_essays.drop_duplicates(subset='REDACAO', inplace=True)\n","df_essays.dropna(inplace=True)\n","df_essays.to_csv(filename, index=False)"],"metadata":{"id":"Upp34Ri3clQ-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_id = \"amadeusai/AV-FI-Qwen2.5-0.5B-PT-BR-Instruct\"\n","\n","# Tokenização do modelo pré-treinado\n","tok = AutoTokenizer.from_pretrained(model_id)\n","# Questão de segurança para modelos criptografados\n","if tok.pad_token is None: tok.pad_token = tok.eos_token\n","\n","# Carregamento do modelo\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_id,\n","    device_map=\"auto\",\n","    dtype=\"auto\",\n","    trust_remote_code=True # Permite o uso de modelos com arquiteturas mais recentes que a biblioteca não conhece\n",")\n","model"],"metadata":{"id":"6F8cperPc2o-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lora_cfg = LoraConfig(\n","    r=8, # Maior capacidade de aprender detalhes complexos devido a um número maior de parâmetros\n","    lora_alpha=16, # Força do impacto do que o LoRA aprendeu sobre o conhecimento original do modelo (valor alto = novo treino se sobrepõe ao modelo)\n","    lora_dropout=0.05, # A cada passo do treino, 5% dos neurônios do LoRA são \"desligados\"\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"]\n",")\n","\n","# Envolve o modelo base com o LoRA\n","model = get_peft_model(model, lora_cfg)\n","model"],"metadata":{"id":"Xt3e5I3ddAcF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Template para prompt\n","IN_FMT = \"\"\"Corrija a redação do ENEM.\\n\\nTema: {tema}\\n\\nRedação:\\n{redacao}\\n\\nFeedback:\\n\"\"\"\n","\n","# Conversão do DataFrame para um dataset do tipo Hugging Face\n","ds = Dataset.from_pandas(df_essays)\n","ds"],"metadata":{"id":"lMGl8dLYdJgl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tamanho máximo da sequência\n","MAX_LEN = 512\n","\n","def tok_ex(ex):\n","  # Formatação da parte de instrução do prompt\n","  prompt = IN_FMT.format(tema=ex['THEME'], redacao=ex['ESSAY'])\n","  # Converte o texto do prompt em tokens sem adição de marcadores de início e fim de frase\n","  tp = tok(prompt, add_special_tokens=False)\n","  # Converte o texto da resposta em tokens sem adição de marcadores de início e fim de frase\n","  tr = tok(ex['COMMENTS'] + tok.eos_token, add_special_tokens=False)\n","  # Concatenar tokens de prompt e de resposta\n","  ids = (tp['input_ids'] + tr['input_ids'])[:MAX_LEN]\n","  # Criação da máscara de atenção\n","  att = [1] * len(ids)\n","  # Criação dos labels (modelo não deve treinar no prompt, apenas na resposta)\n","  lab = ([-100]*len(tp[\"input_ids\"]) + tr[\"input_ids\"])[:MAX_LEN] # -100 no prompt: o cálculo de loss dessa parte é ignorado\n","  # Exemplo tokenizado com os IDs de entrada, máscara de atenção e labels\n","  return {\"input_ids\": ids, \"attention_mask\": att, \"labels\": lab}\n","\n","train_ds = ds.map(tok_ex, remove_columns=ds.column_names)\n","train_ds"],"metadata":{"id":"nw6jZ6ModNzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# batch: lista com exemplos do dataset\n","def collate(batch):\n","  # Função auxiliar para auxiliar na organização das redações\n","  def pad_list(lst, pad_val):\n","    # Maior texto\n","    maxlen = max(len(x) for x in lst)\n","\n","    out = []\n","    # Com os textos menores, adiciona um padding ao final até que fiquem do tamanho do maior\n","    for x in lst:\n","      # Conversão para um tensor\n","      t = torch.tensor(x, dtype=torch.long)\n","      # Adição do padding\n","      if t.size(0) < maxlen:\n","        t = torch.nn.functional.pad(t, (0, maxlen - t.size(0)), value=pad_val)\n","      out.append(t)\n","\n","    # Transforma a lista em uma matriz (tensor)\n","    return torch.stack(out)\n","\n","  # Extrai os input IDs de todos os exemplos no batch\n","  ids = [b[\"input_ids\"] for b in batch]\n","  # Extrai as máscaras de atenção de todos os exemplos no batch\n","  att = [b[\"attention_mask\"] for b in batch]\n","  # Extrai os labels de todos os exemplos no batch\n","  lab = [b[\"labels\"] for b in batch]\n","\n","  # Texto (input_ids)\n","  # Preenche o espaço vazio com um token nulo para o modelo saber que aquilo não é uma palavra\n","\n","  # Atenção (attention_mask)\n","  # 0 para ignorar, caso contrário, o modelo tentará ler um espaço em branco\n","\n","  # Gabarito (labels)\n","  # -100 para ignorar, assim o modelo não é avaliado por acertar um \"espaço em branco\"\n","  return{\n","      \"input_ids\": pad_list(ids, tok.pad_token_id),\n","      \"attention_mask\": pad_list(att, 0),\n","      \"labels\": pad_list(lab, -100)\n","  }"],"metadata":{"id":"_mtLoFkddfGl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["args = TrainingArguments(\n","    output_dir=\"./redacao\",\n","    num_train_epochs=2, # Número de épocas = quantidade de vezes que o dataset é apresentado para o modelo\n","    per_device_train_batch_size=2, # Tamanho da lista (batch) por GPU\n","    gradient_accumulation_steps=4, # Batchs efetivos = 16\n","    learning_rate=2e-4, # Taxa de aprendizado = velocidade que o modelo aprende\n","    logging_steps=10, # Frequência com que o treinamento imprime os resultados\n","    save_steps=50,\n","    report_to=\"none\", # Desativa o envio de relatórios para ferramentas externas\n","    bf16=True, # GPU otimizada para trabalhar com 16 bits, acelerando cálculos\n","    optim=\"paged_adamw_32bit\", # Evita que o treino seja interrompido for falta de recursos (quando a memória chega no limite)\n","    group_by_length=True # Organiza os batch para que textos de tamanhos parecidos fiquem juntos\n",")\n","\n","trainer = Trainer(model=model, args=args, train_dataset=train_ds, data_collator=collate)\n","trainer.model.config"],"metadata":{"id":"MQW-PEeodhxV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"Mxai89-WdmPN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def avaliar_geracao(original, generated):\n","  reference = [word_tokenize(original, language='portuguese')]\n","  candidate = word_tokenize(generated, language='portuguese')\n","\n","  score = sentence_bleu(reference, candidate)\n","  return score"],"metadata":{"id":"L1R1LZS_dp6V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generate(instr):\n","  prompt = IN_FMT.format(tema=instr['THEME'], redacao=instr['ESSAY'])\n","  x = tok(prompt, return_tensors=\"pt\").to(model.device)\n","\n","  with torch.no_grad():\n","    y = model.generate(\n","        **x,\n","        max_new_tokens=1024,\n","        do_sample=False,\n","        eos_token_id=tok.eos_token_id,\n","        pad_token_id=tok.pad_token_id,\n","        no_repeat_ngram_size=3\n","    )\n","\n","  # Calculamos o tamanho do prompt original para cortá-lo da resposta\n","  input_len = x[\"input_ids\"].shape[1]\n","  # Pegamos apenas os tokens que vieram DEPOIS do prompt\n","  generated_tokens = y[0][input_len:]\n","  # Decodificamos apenas a resposta nova\n","  generated_text = tok.decode(generated_tokens, skip_special_tokens=True)\n","  # Pegamos o gabarito original\n","  original_text = instr['COMMENTS']\n","\n","  score = avaliar_geracao(original_text, generated_text)\n","  print(\"=\"*40)\n","  print(f\"TEMA: {instr['TEMA']}\")\n","  print(\"-\" * 40)\n","  print(\"SAÍDA DO MODELO (Gerado):\")\n","  print(generated_text)\n","  print(\"-\" * 40)\n","  print(\"GABARITO (Esperado):\")\n","  print(original_text)\n","  print(\"=\"*40)\n","  print(f\">> BLEU Score: {score:.3f}\")\n","\n","  return score"],"metadata":{"id":"0J6Cht2jd6kE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["exemplo = df_redacoes.iloc[5]\n","score = generate(exemplo)"],"metadata":{"id":"9BAKV2rMfevs"},"execution_count":null,"outputs":[]}]}